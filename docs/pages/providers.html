<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Providers — Swival</title>
    <meta name="description" content="Providers — Swival documentation">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Providers — Swival">
    <meta property="og:description" content="Providers — Swival documentation">
    <meta property="og:image" content="https://swival.dev/img/logo.png">
    <meta property="og:url" content="https://swival.dev/pages/providers.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Providers — Swival">
    <meta name="twitter:description" content="Providers — Swival documentation">
    <meta name="twitter:image" content="https://swival.dev/img/logo.png">
    <link rel="icon" href="../favicon.ico">
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="header-inner">
            <a href="../" class="header-logo">
                <img src="../img/logo.png" alt="Swival">
            </a>
            <nav class="header-nav">
                <a href="./">Docs</a>
                <a href="https://github.com/swival/swival">GitHub</a>
            </nav>
        </div>
    </header>
    <div class="docs-layout">
        <aside class="sidebar">
            <div class="sidebar-section">
<h4>Basics</h4>
<ul>
<li><a href="getting-started.html">Getting Started</a></li>
<li><a href="usage.html">Usage</a></li>
<li><a href="tools.html">Tools</a></li>
</ul>
</div>
<div class="sidebar-section">
<h4>Configuration & Deployment</h4>
<ul>
<li><a href="safety-and-sandboxing.html">Safety & Sandboxing</a></li>
<li><a href="skills.html">Skills</a></li>
<li><a href="mcp.html">MCP</a></li>
<li><a href="customization.html">Customization</a></li>
<li><a href="providers.html" class="active">Providers</a></li>
<li><a href="reports.html">Reports</a></li>
<li><a href="reviews.html">Reviews</a></li>
<li><a href="agentfs.html">AgentFS</a></li>
</ul>
</div>
        </aside>
        <article class="docs-content">
            <h1 id="providers">Providers</h1>
<p>Swival supports LM Studio for local inference, HuggingFace Inference API for hosted inference, and OpenRouter for multi-provider access through a single API. All provider calls are normalized through <a href="https://docs.litellm.ai/">LiteLLM</a>, so the runtime loop stays consistent while credential and model routing change per provider.</p>
<h2 id="lm-studio">LM Studio</h2>
<p>LM Studio is the default provider and usually requires no flags when the local server is already running with a loaded model.</p>
<p>At startup, Swival calls <code>http://127.0.0.1:1234/api/v1/models</code> unless you override <code>--base-url</code>. It looks for the first model entry with <code>type: "llm"</code> and a non-empty <code>loaded_instances</code> array, then extracts the model identifier and current context length from that payload. If no loaded model is found, Swival exits and asks you to load a model or pass <code>--model</code> explicitly.</p>
<p>If LM Studio is running on another host or port, set <code>--base-url</code>.</p>
<pre><code class="language-sh">swival --base-url http://192.168.1.100:1234 &quot;task&quot;
</code></pre>
<p>If you want to bypass auto-discovery, pass <code>--model</code>.</p>
<pre><code class="language-sh">swival --model &quot;qwen3-coder-next&quot; &quot;task&quot;
</code></pre>
<p>If you pass <code>--max-context-tokens</code>, Swival may reload the model through LM Studio's <code>/api/v1/models/load</code> endpoint.</p>
<pre><code class="language-sh">swival --max-context-tokens 131072 &quot;task&quot;
</code></pre>
<p>If the requested value already matches the loaded context length, no reload happens.
When a reload is required, it can take noticeable time depending on model size and hardware.</p>
<p>Internally, LM Studio calls are routed through LiteLLM as an OpenAI-compatible endpoint. Swival sends the model as <code>openai/&lt;model_id&gt;</code>, sets <code>api_base</code> to <code>&lt;base_url&gt;/v1</code>, and uses the placeholder API key <code>lm-studio</code>.</p>
<h2 id="huggingface-inference-api">HuggingFace Inference API</h2>
<p>For HuggingFace, <code>--model</code> is required and must be in <code>org/model</code> format. Authentication comes from <code>HF_TOKEN</code> by default or <code>--api-key</code> if you pass one explicitly.</p>
<pre><code class="language-sh">export HF_TOKEN=hf_your_token_here
swival --provider huggingface --model zai-org/GLM-5 &quot;task&quot;
</code></pre>
<p>Serverless HuggingFace endpoints often expose smaller context windows than local deployments, so long multi-turn coding sessions can hit context pressure sooner.</p>
<p>For dedicated endpoints, keep the same model identifier and pass your endpoint URL and key.</p>
<pre><code class="language-sh">swival --provider huggingface \
    --model zai-org/GLM-5 \
    --base-url https://xyz.endpoints.huggingface.cloud \
    --api-key hf_your_key \
    &quot;task&quot;
</code></pre>
<p>Internally, Swival normalizes the model to <code>huggingface/&lt;model_id&gt;</code> for LiteLLM and strips an existing <code>huggingface/</code> prefix if you already included it. If <code>--base-url</code> is set, it is forwarded as <code>api_base</code>.
Dedicated endpoints usually let you use the full deployed model context window rather than tighter serverless limits.</p>
<h2 id="openrouter">OpenRouter</h2>
<p>For OpenRouter, <code>--model</code> is required and authentication comes from <code>OPENROUTER_API_KEY</code> or <code>--api-key</code>.</p>
<pre><code class="language-sh">export OPENROUTER_API_KEY=sk_or_your_token_here
swival --provider openrouter --model openrouter/free &quot;task&quot;
</code></pre>
<p>If you use an OpenRouter-compatible custom endpoint, set <code>--base-url</code>.</p>
<pre><code class="language-sh">swival --provider openrouter \
    --model openrouter/free \
    --base-url https://custom.openrouter.endpoint \
    --api-key sk_or_key \
    &quot;task&quot;
</code></pre>
<p>OpenRouter models vary widely in context limits, so you should set <code>--max-context-tokens</code> to match the model you chose.</p>
<pre><code class="language-sh">swival --provider openrouter --model openrouter/free \
    --max-context-tokens 131072 &quot;task&quot;
</code></pre>
<p>Internally, Swival normalizes OpenRouter models to LiteLLM's <code>openrouter/...</code> format. If you already pass a fully prefixed value like <code>openrouter/openrouter/free</code>, Swival keeps it stable instead of adding another prefix.</p>
<h2 id="adding-more-providers-later">Adding More Providers Later</h2>
<p>Because API calls are already abstracted behind LiteLLM, adding a provider is mostly a matter of argument validation, model normalization, and credential wiring. The provider-specific branch in <code>call_llm()</code> is intentionally compact so new providers can be added without changing the rest of the agent loop.
In practice, each provider branch is only about ten lines of routing logic.</p>
        </article>
    </div>
    <footer class="site-footer">
        MIT License &middot;
        <a href="https://github.com/swival/swival">GitHub</a>
    </footer>
</body>
</html>