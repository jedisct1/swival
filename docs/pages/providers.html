<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Providers — Swival</title>
    <meta name="description" content="Providers — Swival documentation">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Providers — Swival">
    <meta property="og:description" content="Providers — Swival documentation">
    <meta property="og:image" content="https://swival.dev/img/logo.png">
    <meta property="og:url" content="https://swival.dev/pages/providers.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Providers — Swival">
    <meta name="twitter:description" content="Providers — Swival documentation">
    <meta name="twitter:image" content="https://swival.dev/img/logo.png">
    <link rel="icon" href="../favicon.ico">
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="header-inner">
            <a href="../" class="header-logo">
                <img src="../img/logo.png" alt="Swival">
            </a>
            <nav class="header-nav">
                <a href="./">Docs</a>
                <a href="https://github.com/swival/swival">GitHub</a>
            </nav>
        </div>
    </header>
    <div class="docs-layout">
        <aside class="sidebar">
            <div class="sidebar-section">
<h4>Basics</h4>
<ul>
<li><a href="getting-started.html">Getting Started</a></li>
<li><a href="usage.html">Usage</a></li>
<li><a href="tools.html">Tools</a></li>
</ul>
</div>
<div class="sidebar-section">
<h4>Configuration & Deployment</h4>
<ul>
<li><a href="safety-and-sandboxing.html">Safety & Sandboxing</a></li>
<li><a href="skills.html">Skills</a></li>
<li><a href="mcp.html">MCP</a></li>
<li><a href="customization.html">Customization</a></li>
<li><a href="providers.html" class="active">Providers</a></li>
<li><a href="reports.html">Reports</a></li>
<li><a href="reviews.html">Reviews</a></li>
<li><a href="agentfs.html">AgentFS</a></li>
</ul>
</div>
        </aside>
        <article class="docs-content">
            <h1 id="providers">Providers</h1>
<p>Swival supports LM Studio for local inference, HuggingFace Inference API for hosted inference, OpenRouter for multi-provider access through a single API, and a generic provider for any OpenAI-compatible server. All provider calls are normalized through <a href="https://docs.litellm.ai/">LiteLLM</a>, so the runtime loop stays consistent while credential and model routing change per provider.</p>
<h2 id="lm-studio">LM Studio</h2>
<p>LM Studio is the default provider and usually requires no flags when the local server is already running with a loaded model.</p>
<p>At startup, Swival calls <code>http://127.0.0.1:1234/api/v1/models</code> unless you override <code>--base-url</code>. It looks for the first model entry with <code>type: "llm"</code> and a non-empty <code>loaded_instances</code> array, then extracts the model identifier and current context length from that payload. If no loaded model is found, Swival exits and asks you to load a model or pass <code>--model</code> explicitly.</p>
<p>If LM Studio is running on another host or port, set <code>--base-url</code>.</p>
<pre><code class="language-sh">swival --base-url http://192.168.1.100:1234 &quot;task&quot;
</code></pre>
<p>If you want to bypass auto-discovery, pass <code>--model</code>.</p>
<pre><code class="language-sh">swival --model &quot;qwen3-coder-next&quot; &quot;task&quot;
</code></pre>
<p>If you pass <code>--max-context-tokens</code>, Swival may reload the model through LM Studio's <code>/api/v1/models/load</code> endpoint.</p>
<pre><code class="language-sh">swival --max-context-tokens 131072 &quot;task&quot;
</code></pre>
<p>If the requested value already matches the loaded context length, no reload happens.
When a reload is required, it can take noticeable time depending on model size and hardware.</p>
<p>Internally, LM Studio calls are routed through LiteLLM as an OpenAI-compatible endpoint. Swival sends the model as <code>openai/&lt;model_id&gt;</code>, sets <code>api_base</code> to <code>&lt;base_url&gt;/v1</code>, and uses the placeholder API key <code>lm-studio</code>.</p>
<h2 id="huggingface-inference-api">HuggingFace Inference API</h2>
<p>For HuggingFace, <code>--model</code> is required and must be in <code>org/model</code> format. Authentication comes from <code>HF_TOKEN</code> by default or <code>--api-key</code> if you pass one explicitly.</p>
<pre><code class="language-sh">export HF_TOKEN=hf_your_token_here
swival --provider huggingface --model zai-org/GLM-5 &quot;task&quot;
</code></pre>
<p>Serverless HuggingFace endpoints often expose smaller context windows than local deployments, so long multi-turn coding sessions can hit context pressure sooner.</p>
<p>For dedicated endpoints, keep the same model identifier and pass your endpoint URL and key.</p>
<pre><code class="language-sh">swival --provider huggingface \
    --model zai-org/GLM-5 \
    --base-url https://xyz.endpoints.huggingface.cloud \
    --api-key hf_your_key \
    &quot;task&quot;
</code></pre>
<p>Internally, Swival normalizes the model to <code>huggingface/&lt;model_id&gt;</code> for LiteLLM and strips an existing <code>huggingface/</code> prefix if you already included it. If <code>--base-url</code> is set, it is forwarded as <code>api_base</code>.</p>
<p>Dedicated endpoints usually let you use the full deployed model context window rather than tighter serverless limits.</p>
<h2 id="openrouter">OpenRouter</h2>
<p>For OpenRouter, <code>--model</code> is required and authentication comes from <code>OPENROUTER_API_KEY</code> or <code>--api-key</code>.</p>
<pre><code class="language-sh">export OPENROUTER_API_KEY=sk_or_your_token_here
swival --provider openrouter --model z-ai/glm-5 &quot;task&quot;
</code></pre>
<p>If you use an OpenRouter-compatible custom endpoint, set <code>--base-url</code>.</p>
<pre><code class="language-sh">swival --provider openrouter \
    --model z-ai/glm-5 \
    --base-url https://custom.openrouter.endpoint \
    --api-key sk_or_key \
    &quot;task&quot;
</code></pre>
<p>OpenRouter models vary widely in context limits, so you should set <code>--max-context-tokens</code> to match the model you chose.</p>
<pre><code class="language-sh">swival --provider openrouter --model z-ai/glm-5 \
    --max-context-tokens 131072 &quot;task&quot;
</code></pre>
<p>Internally, Swival normalizes OpenRouter models to LiteLLM's <code>openrouter/...</code> format. If the model identifier starts with the literal double prefix <code>openrouter/openrouter/</code>, Swival strips the redundant prefix so LiteLLM does not see a stutter. Any other <code>openrouter/</code> prefix (e.g. <code>openrouter/z-ai/glm-5</code>) is treated as part of the model path and gets prefixed normally, so you should pass bare identifiers like <code>z-ai/glm-5</code>.</p>
<h2 id="generic-openai-compatible">Generic (OpenAI-compatible)</h2>
<p>The generic provider works with any server that exposes an OpenAI-compatible chat completions endpoint. This covers mlx_lm.server, ollama, llama.cpp, vLLM, LocalAI, text-generation-webui, and similar tools.</p>
<p>Both <code>--model</code> and <code>--base-url</code> are required. Pass the server's root URL without <code>/v1</code> — Swival appends it automatically. If your URL already ends in <code>/v1</code>, that's fine too.</p>
<pre><code class="language-sh"># mlx_lm.server
swival --provider generic \
    --base-url http://127.0.0.1:8080 \
    --model mlx-community/Qwen3-Coder-480B-A35B-4bit \
    &quot;task&quot;
</code></pre>
<pre><code class="language-sh"># ollama
swival --provider generic \
    --base-url http://127.0.0.1:11434 \
    --model qwen3:32b \
    &quot;task&quot;
</code></pre>
<pre><code class="language-sh"># llama.cpp server
swival --provider generic \
    --base-url http://127.0.0.1:8080 \
    --model default \
    &quot;task&quot;
</code></pre>
<p>No API key is required for most local servers. If your server needs one, pass <code>--api-key</code> or set <code>OPENAI_API_KEY</code>.</p>
<pre><code class="language-sh">export OPENAI_API_KEY=sk-...
swival --provider generic \
    --base-url https://my-server.example.com \
    --model my-model \
    &quot;task&quot;
</code></pre>
<p>There is no model auto-discovery and no context window reload. Set <code>--max-context-tokens</code> manually if you need Swival to know the window size.</p>
<p>Internally, generic calls are routed through LiteLLM as <code>openai/&lt;model_id&gt;</code> with <code>api_base</code> pointing at your server's <code>/v1</code> path.</p>
<h2 id="extra-provider-parameters">Extra Provider Parameters</h2>
<p>Some models and servers accept parameters that go beyond the standard OpenAI API. Use <code>--extra-body</code> to pass them through. The value is a JSON object that gets forwarded directly to the API call.</p>
<p>For example, Qwen models served through vLLM can disable internal thinking mode:</p>
<pre><code class="language-sh">swival --provider generic \
    --base-url http://127.0.0.1:8000 \
    --model Qwen/Qwen3.5-35B-A3B \
    --extra-body '{&quot;chat_template_kwargs&quot;: {&quot;enable_thinking&quot;: false}}' \
    &quot;task&quot;
</code></pre>
<p>You can also set this in config so you don't repeat it every time:</p>
<pre><code class="language-toml">provider = &quot;generic&quot;
base_url = &quot;http://127.0.0.1:8000&quot;
model = &quot;Qwen/Qwen3.5-35B-A3B&quot;
extra_body = { chat_template_kwargs = { enable_thinking = false } }
</code></pre>
<p>The dictionary is forwarded as <code>extra_body</code> to LiteLLM, which passes it through to the server. Refer to your model or server documentation for supported parameters.</p>
<h2 id="adding-more-providers-later">Adding More Providers Later</h2>
<p>Because API calls are already abstracted behind LiteLLM, adding a provider is mostly a matter of argument validation, model normalization, and credential wiring. The provider-specific branch in <code>call_llm()</code> is intentionally compact so new providers can be added without changing the rest of the agent loop.</p>
<p>In practice, each provider branch is only about ten lines of routing logic.</p>
        </article>
    </div>
    <footer class="site-footer">
        MIT License &middot;
        <a href="https://github.com/swival/swival">GitHub</a>
    </footer>
</body>
</html>