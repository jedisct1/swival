<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Reviews — Swival</title>
    <meta name="description" content="Reviews — Swival documentation">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Reviews — Swival">
    <meta property="og:description" content="Reviews — Swival documentation">
    <meta property="og:image" content="https://swival.github.io/swival/img/logo.png">
    <meta property="og:url" content="https://swival.github.io/swival/pages/reviews.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Reviews — Swival">
    <meta name="twitter:description" content="Reviews — Swival documentation">
    <meta name="twitter:image" content="https://swival.github.io/swival/img/logo.png">
    <link rel="icon" href="../favicon.ico">
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="header-inner">
            <a href="../" class="header-logo">
                <img src="../img/logo.png" alt="Swival">
            </a>
            <nav class="header-nav">
                <a href="./">Docs</a>
                <a href="https://github.com/swival/swival">GitHub</a>
            </nav>
        </div>
    </header>
    <div class="docs-layout">
        <aside class="sidebar">
            <div class="sidebar-section">
<h4>Basics</h4>
<ul>
<li><a href="getting-started.html">Getting Started</a></li>
<li><a href="usage.html">Usage</a></li>
<li><a href="tools.html">Tools</a></li>
</ul>
</div>
<div class="sidebar-section">
<h4>Configuration & Deployment</h4>
<ul>
<li><a href="safety-and-sandboxing.html">Safety & Sandboxing</a></li>
<li><a href="skills.html">Skills</a></li>
<li><a href="customization.html">Customization</a></li>
<li><a href="providers.html">Providers</a></li>
<li><a href="reports.html">Reports</a></li>
<li><a href="reviews.html" class="active">Reviews</a></li>
<li><a href="agentfs.html">AgentFS</a></li>
</ul>
</div>
        </aside>
        <article class="docs-content">
            <h1 id="reviews">Reviews</h1>
<p>The <code>--reviewer</code> flag lets you attach an external executable to the agent loop. After Swival produces an answer, it runs that executable, passes the answer on standard input, and decides whether to accept or retry based on the reviewer's exit code.</p>
<pre><code class="language-sh">swival &quot;Refactor the error handling in src/api.py&quot; --reviewer ./review.sh
</code></pre>
<p>This pattern works well for automated gates such as tests, linting, format checks, schema checks, or LLM-as-a-judge scoring. <code>--reviewer</code> is incompatible with <code>--repl</code>.</p>
<h2 id="reviewer-protocol">Reviewer Protocol</h2>
<p>Swival invokes the reviewer as <code>reviewer_executable &lt;base_dir&gt;</code>. The first positional argument is the absolute base directory. The full assistant answer is written to reviewer standard input.</p>
<p>If the reviewer exits with code <code>0</code>, Swival accepts the answer immediately and ends normally, and reviewer standard output is ignored. If the reviewer exits with code <code>1</code>, Swival treats reviewer standard output as feedback, appends that feedback as a new user message, resets turn budget for a new pass, and continues the loop. If the reviewer exits with code <code>2</code>, Swival treats that as reviewer failure, warns on standard error when diagnostics are enabled, and accepts the current answer unchanged while ignoring reviewer standard output. Any other nonzero exit code is handled the same way as <code>2</code>.</p>
<p>Reviewer execution has a 120-second timeout. Timeout or spawn failures are treated as reviewer errors and do not discard the agent's answer.</p>
<h2 id="reviewer-environment-variables">Reviewer Environment Variables</h2>
<p>Swival sets context variables on the reviewer subprocess for each round. <code>SWIVAL_TASK</code> contains the original user task and is always set. <code>SWIVAL_REVIEW_ROUND</code> contains the current review round number and is always set. <code>SWIVAL_MODEL</code> contains the resolved model identifier when available.</p>
<p>The reviewer inherits the parent environment too, but Swival's injected values override any same-named parent values.</p>
<h2 id="writing-a-reviewer-script">Writing A Reviewer Script</h2>
<p>A minimal reviewer that accepts only when tests pass can look like this:</p>
<pre><code class="language-bash">#!/usr/bin/env bash
set -euo pipefail

base_dir=&quot;$1&quot;
cd &quot;$base_dir&quot;

if python3 -m pytest tests/ -q 2&gt;&amp;1; then
    exit 0
else
    echo &quot;Tests are failing. Fix the test failures and try again.&quot;
    exit 1
fi
</code></pre>
<p>A reviewer that requires valid JSON output can look like this:</p>
<pre><code class="language-bash">#!/usr/bin/env bash
set -euo pipefail

answer=$(cat)

if echo &quot;$answer&quot; | python3 -c &quot;import sys, json; json.load(sys.stdin)&quot; 2&gt;/dev/null; then
    exit 0
else
    echo &quot;Your answer is not valid JSON. Please output only valid JSON.&quot;
    exit 1
fi
</code></pre>
<p>The reviewer file must exist and be executable. Swival validates this before the run starts.</p>
<h2 id="using-swival-as-the-reviewer">Using Swival As The Reviewer</h2>
<p>You can run a second Swival instance as an LLM judge. The outer Swival does the work and the inner Swival evaluates quality against the original task and current answer.</p>
<pre><code class="language-bash">#!/usr/bin/env bash
# judge.sh -- use a second Swival instance to review the agent's answer
set -uo pipefail

base_dir=&quot;$1&quot;
answer=$(cat)

judge_output=$(swival &quot;You are reviewing a coding agent's output.

&lt;task&gt;$SWIVAL_TASK&lt;/task&gt;

&lt;answer&gt;$answer&lt;/answer&gt;

Evaluate whether the answer correctly and completely addresses the task.
Respond with exactly one of:
  VERDICT: ACCEPT
  VERDICT: RETRY followed by your feedback on the next line.&quot; \
    --base-dir &quot;$base_dir&quot; --max-turns 3 --quiet --no-history 2&gt;/dev/null)
judge_exit=$?

if [ $judge_exit -eq 1 ] || [ -z &quot;$judge_output&quot; ]; then
    exit 2
fi

if echo &quot;$judge_output&quot; | grep -qi &quot;VERDICT: ACCEPT&quot;; then
    exit 0
elif echo &quot;$judge_output&quot; | grep -qi &quot;VERDICT: RETRY&quot;; then
    echo &quot;$judge_output&quot; | sed '1,/VERDICT: RETRY/d'
    exit 1
else
    exit 2
fi
</code></pre>
<p>This wrapper keeps reviewer behavior predictable even when the judge fails to return parseable output. In that case, returning exit code <code>2</code> tells the outer run to accept the current answer rather than failing hard.</p>
<p>If both instances point at the same provider, they will use the same backend by default. You can direct the inner judge to a different provider or model by adding <code>--provider</code>, <code>--model</code>, and optionally <code>--base-url</code> inside the wrapper.</p>
<p>Using <code>--max-turns 3</code> for the inner judge is practical because some models spend a turn on tool-based reasoning before producing a verdict. Using <code>set -uo pipefail</code> instead of <code>set -euo pipefail</code> avoids aborting the wrapper early when the inner run exits with code <code>2</code> but still returns parseable output.</p>
<h2 id="retry-and-round-limits">Retry And Round Limits</h2>
<p>Every time the reviewer returns exit code <code>1</code>, Swival appends reviewer feedback as a user message and re-enters the loop with a fresh turn budget. The full conversation stays intact, so the model can build on prior work instead of restarting from scratch.</p>
<p>To prevent infinite cycles, Swival enforces a hard limit of five review rounds. If round five still returns code <code>1</code>, Swival accepts the latest answer and emits a warning when diagnostics are enabled.</p>
<h2 id="failure-handling">Failure Handling</h2>
<p>Startup validation fails fast if the reviewer executable is missing or non-executable.</p>
<p>After startup, reviewer failures are non-fatal. Timeout failures, process spawn failures, and crash-style exits all degrade to reviewer error handling, which means the current answer is accepted and returned.</p>
<h2 id="interaction-with-quiet-and-report">Interaction With <code>--quiet</code> And <code>--report</code></h2>
<p>With <code>--quiet</code>, reviewer diagnostics are suppressed along with other diagnostic logging. Rejected intermediate answers are not printed to standard output; only the final accepted answer is printed.</p>
<p>With <code>--report</code>, review rounds are captured in the timeline and <code>stats.review_rounds</code> records how many reviewer invocations occurred. Turn numbers remain cumulative across rounds, so the timeline reads as one continuous run.</p>
<pre><code class="language-sh">jq '.stats.review_rounds' report.json
jq '.timeline[] | {turn, type}' report.json
</code></pre>
<h2 id="example-ci-flow">Example CI Flow</h2>
<p>A simple CI-style invocation can combine command access, reviewer retries, and report capture in one run.</p>
<pre><code class="language-sh">swival &quot;Fix the failing tests in tests/unit/&quot; \
    --allowed-commands python3,pytest \
    --reviewer ./ci-review.sh \
    --report results.json \
    --quiet
</code></pre>
<p>In that setup, <code>ci-review.sh</code> should return <code>0</code> when checks pass and return <code>1</code> with actionable feedback when checks fail. Swival will retry with that feedback up to five rounds.</p>
        </article>
    </div>
    <footer class="site-footer">
        MIT License &middot;
        <a href="https://github.com/swival/swival">GitHub</a>
    </footer>
</body>
</html>