<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Reports — Swival</title>
    <meta name="description" content="Reports — Swival documentation">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Reports — Swival">
    <meta property="og:description" content="Reports — Swival documentation">
    <meta property="og:image" content="https://swival.github.io/swival/img/logo.png">
    <meta property="og:url" content="https://swival.github.io/swival/pages/reports.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Reports — Swival">
    <meta name="twitter:description" content="Reports — Swival documentation">
    <meta name="twitter:image" content="https://swival.github.io/swival/img/logo.png">
    <link rel="icon" href="../favicon.ico">
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="header-inner">
            <a href="../" class="header-logo">
                <img src="../img/logo.png" alt="Swival">
            </a>
            <nav class="header-nav">
                <a href="./">Docs</a>
                <a href="https://github.com/swival/swival">GitHub</a>
            </nav>
        </div>
    </header>
    <div class="docs-layout">
        <aside class="sidebar">
            <div class="sidebar-section">
<h4>Basics</h4>
<ul>
<li><a href="getting-started.html">Getting Started</a></li>
<li><a href="usage.html">Usage</a></li>
<li><a href="tools.html">Tools</a></li>
</ul>
</div>
<div class="sidebar-section">
<h4>Configuration & Deployment</h4>
<ul>
<li><a href="safety-and-sandboxing.html">Safety & Sandboxing</a></li>
<li><a href="skills.html">Skills</a></li>
<li><a href="customization.html">Customization</a></li>
<li><a href="providers.html">Providers</a></li>
<li><a href="reports.html" class="active">Reports</a></li>
<li><a href="reviews.html">Reviews</a></li>
<li><a href="agentfs.html">AgentFS</a></li>
</ul>
</div>
        </aside>
        <article class="docs-content">
            <h1 id="reports">Reports</h1>
<p>The <code>--report</code> flag produces a JSON file that captures everything that happened
during an agent run: the outcome, timing, tool usage, context management events,
and a full action-by-action timeline. It's designed for benchmarking and
evaluation -- comparing models, tuning settings, or measuring how well an
AGENT.md file guides the agent on a set of tasks.</p>
<pre><code class="language-sh">swival &quot;Refactor the error handling in src/api.py&quot; --report run1.json
</code></pre>
<p>When <code>--report</code> is active, the final answer goes into the JSON file instead of
stdout. Diagnostic output on stderr is unaffected.</p>
<p><code>--report</code> is incompatible with <code>--repl</code>.</p>
<h2 id="example">Example</h2>
<p>Here's a real report from a simple task. The agent called <code>python3</code> to compute
a square root, then delivered the answer:</p>
<pre><code class="language-json">{
  &quot;version&quot;: 1,
  &quot;timestamp&quot;: &quot;2026-02-25T10:36:28.312457+00:00&quot;,
  &quot;task&quot;: &quot;Compute the square root of 23847234&quot;,
  &quot;model&quot;: &quot;qwen3-coder-next&quot;,
  &quot;provider&quot;: &quot;lmstudio&quot;,
  &quot;settings&quot;: {
    &quot;temperature&quot;: 0.55,
    &quot;top_p&quot;: 1.0,
    &quot;seed&quot;: null,
    &quot;max_turns&quot;: 100,
    &quot;max_output_tokens&quot;: 32768,
    &quot;context_length&quot;: 100000,
    &quot;yolo&quot;: false,
    &quot;allowed_commands&quot;: [
      &quot;python3&quot;,
      &quot;ruby&quot;
    ],
    &quot;skills_discovered&quot;: [],
    &quot;instructions_loaded&quot;: [
      &quot;AGENT.md&quot;
    ]
  },
  &quot;result&quot;: {
    &quot;outcome&quot;: &quot;success&quot;,
    &quot;answer&quot;: &quot;The square root of 23,847,234 is approximately **4,883.36**.&quot;,
    &quot;exit_code&quot;: 0
  },
  &quot;stats&quot;: {
    &quot;turns&quot;: 2,
    &quot;tool_calls_total&quot;: 1,
    &quot;tool_calls_succeeded&quot;: 1,
    &quot;tool_calls_failed&quot;: 0,
    &quot;tool_calls_by_name&quot;: {
      &quot;run_command&quot;: {
        &quot;succeeded&quot;: 1,
        &quot;failed&quot;: 0
      }
    },
    &quot;compactions&quot;: 0,
    &quot;turn_drops&quot;: 0,
    &quot;guardrail_interventions&quot;: 0,
    &quot;truncated_responses&quot;: 0,
    &quot;llm_calls&quot;: 2,
    &quot;total_llm_time_s&quot;: 48.25,
    &quot;total_tool_time_s&quot;: 0.082,
    &quot;skills_used&quot;: []
  },
  &quot;timeline&quot;: [
    {
      &quot;turn&quot;: 1,
      &quot;type&quot;: &quot;llm_call&quot;,
      &quot;duration_s&quot;: 43.831,
      &quot;prompt_tokens_est&quot;: 4758,
      &quot;finish_reason&quot;: &quot;tool_calls&quot;,
      &quot;is_retry&quot;: false
    },
    {
      &quot;turn&quot;: 1,
      &quot;type&quot;: &quot;tool_call&quot;,
      &quot;name&quot;: &quot;run_command&quot;,
      &quot;arguments&quot;: {
        &quot;command&quot;: [
          &quot;python3&quot;,
          &quot;-c&quot;,
          &quot;import math; print(math.sqrt(23847234))&quot;
        ]
      },
      &quot;succeeded&quot;: true,
      &quot;duration_s&quot;: 0.082,
      &quot;result_length&quot;: 18
    },
    {
      &quot;turn&quot;: 2,
      &quot;type&quot;: &quot;llm_call&quot;,
      &quot;duration_s&quot;: 4.419,
      &quot;prompt_tokens_est&quot;: 4797,
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;is_retry&quot;: false
    }
  ]
}
</code></pre>
<h2 id="report-structure">Report structure</h2>
<h3 id="top-level-fields">Top-level fields</h3>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>version</code></td>
<td>int</td>
<td>Schema version (currently <code>1</code>)</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>string</td>
<td>UTC ISO 8601 timestamp of when the run ended</td>
</tr>
<tr>
<td><code>task</code></td>
<td>string</td>
<td>The question/task passed on the command line</td>
</tr>
<tr>
<td><code>model</code></td>
<td>string</td>
<td>Model identifier (auto-discovered or from CLI)</td>
</tr>
<tr>
<td><code>provider</code></td>
<td>string</td>
<td><code>lmstudio</code>, <code>huggingface</code>, or <code>openrouter</code></td>
</tr>
<tr>
<td><code>settings</code></td>
<td>object</td>
<td>Configuration snapshot (see below)</td>
</tr>
<tr>
<td><code>result</code></td>
<td>object</td>
<td>Outcome and answer (see below)</td>
</tr>
<tr>
<td><code>stats</code></td>
<td>object</td>
<td>Aggregate counters (see below)</td>
</tr>
<tr>
<td><code>timeline</code></td>
<td>array</td>
<td>Ordered list of every event (see below)</td>
</tr>
</tbody>
</table>
<h3 id="settings">settings</h3>
<p>A snapshot of the configuration used for the run.</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>temperature</code></td>
<td>float</td>
<td>Sampling temperature</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td>float</td>
<td>Top-p (nucleus) sampling</td>
</tr>
<tr>
<td><code>seed</code></td>
<td>int or null</td>
<td>Random seed, if set</td>
</tr>
<tr>
<td><code>max_turns</code></td>
<td>int</td>
<td>Maximum agent loop iterations</td>
</tr>
<tr>
<td><code>max_output_tokens</code></td>
<td>int</td>
<td>Max tokens per LLM response</td>
</tr>
<tr>
<td><code>context_length</code></td>
<td>int or null</td>
<td>Effective context window size</td>
</tr>
<tr>
<td><code>yolo</code></td>
<td>bool</td>
<td>Whether YOLO mode was active</td>
</tr>
<tr>
<td><code>allowed_commands</code></td>
<td>string[]</td>
<td>Whitelisted command basenames (sorted)</td>
</tr>
<tr>
<td><code>skills_discovered</code></td>
<td>string[]</td>
<td>Skill names found at startup (sorted)</td>
</tr>
<tr>
<td><code>instructions_loaded</code></td>
<td>string[]</td>
<td>Instruction files loaded (<code>CLAUDE.md</code>, etc.)</td>
</tr>
</tbody>
</table>
<h3 id="result">result</h3>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>outcome</code></td>
<td>string</td>
<td><code>success</code>, <code>exhausted</code>, or <code>error</code></td>
</tr>
<tr>
<td><code>answer</code></td>
<td>string/null</td>
<td>The agent's final text answer (null on error)</td>
</tr>
<tr>
<td><code>exit_code</code></td>
<td>int</td>
<td>Process exit code (0, 1, or 2)</td>
</tr>
<tr>
<td><code>error_message</code></td>
<td>string</td>
<td>Present only when <code>outcome</code> is <code>error</code></td>
</tr>
</tbody>
</table>
<p>Outcomes:</p>
<ul>
<li><strong>success</strong> -- the agent produced a final answer.</li>
<li><strong>exhausted</strong> -- the agent hit <code>max_turns</code> without finishing. <code>exit_code</code> is 2.</li>
<li><strong>error</strong> -- a runtime failure (couldn't connect, context overflow after all
  recovery attempts, bad model config, etc.). <code>exit_code</code> is 1.</li>
</ul>
<h3 id="stats">stats</h3>
<p>Aggregate counters for the entire run.</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>turns</code></td>
<td>int</td>
<td>Number of turns completed</td>
</tr>
<tr>
<td><code>llm_calls</code></td>
<td>int</td>
<td>Total LLM API calls (including retries)</td>
</tr>
<tr>
<td><code>total_llm_time_s</code></td>
<td>float</td>
<td>Wall-clock seconds spent in LLM calls</td>
</tr>
<tr>
<td><code>total_tool_time_s</code></td>
<td>float</td>
<td>Wall-clock seconds spent executing tools</td>
</tr>
<tr>
<td><code>tool_calls_total</code></td>
<td>int</td>
<td>Total tool invocations</td>
</tr>
<tr>
<td><code>tool_calls_succeeded</code></td>
<td>int</td>
<td>Tool calls that returned a result</td>
</tr>
<tr>
<td><code>tool_calls_failed</code></td>
<td>int</td>
<td>Tool calls that returned an error</td>
</tr>
<tr>
<td><code>tool_calls_by_name</code></td>
<td>object</td>
<td>Per-tool breakdown: <code>{"read_file": {"succeeded": 5, "failed": 0}, ...}</code></td>
</tr>
<tr>
<td><code>compactions</code></td>
<td>int</td>
<td>Context compactions (truncating old results)</td>
</tr>
<tr>
<td><code>turn_drops</code></td>
<td>int</td>
<td>Aggressive context recovery (dropping turns)</td>
</tr>
<tr>
<td><code>guardrail_interventions</code></td>
<td>int</td>
<td>Times the guardrail injected corrective messages</td>
</tr>
<tr>
<td><code>truncated_responses</code></td>
<td>int</td>
<td>LLM responses cut short by output token limit</td>
</tr>
<tr>
<td><code>skills_used</code></td>
<td>string[]</td>
<td>Skill names that were successfully activated during the run</td>
</tr>
<tr>
<td><code>review_rounds</code></td>
<td>int</td>
<td>Number of times the external reviewer was invoked (0 if no <code>--reviewer</code>)</td>
</tr>
<tr>
<td><code>todo</code></td>
<td>object</td>
<td>Present only if the todo tool was used: <code>{"added": N, "completed": N, "remaining": N}</code></td>
</tr>
</tbody>
</table>
<h3 id="timeline">timeline</h3>
<p>An ordered array of every event. Each entry has a <code>turn</code> number and a <code>type</code>.</p>
<p><strong><code>llm_call</code></strong> -- one per LLM API invocation, including failed attempts and
retries after context overflow. The example above shows two: the first with
<code>finish_reason: "tool_calls"</code> (the model decided to call a tool) and the second
with <code>"stop"</code> (the model produced its final answer). When a call is a retry
after compaction, <code>is_retry</code> is <code>true</code> and <code>retry_reason</code> is present
(<code>"compact_messages"</code> or <code>"drop_middle_turns"</code>). Failed calls use
<code>finish_reason</code> values like <code>"context_overflow"</code> or <code>"error"</code>.</p>
<p><strong><code>tool_call</code></strong> -- one per tool invocation. The example shows a <code>run_command</code>
call with the full argument list, timing, and result size. <code>arguments</code> is <code>null</code>
when the model produced invalid JSON. <code>error</code> is present when <code>succeeded</code> is
<code>false</code>.</p>
<p><strong><code>compaction</code></strong> -- context recovery. Recorded when the context window fills up
and Swival has to trim history. <code>strategy</code> is <code>"compact_messages"</code> (truncating
old tool results) or <code>"drop_middle_turns"</code> (removing entire middle turns).</p>
<pre><code class="language-json">{&quot;turn&quot;: 15, &quot;type&quot;: &quot;compaction&quot;, &quot;strategy&quot;: &quot;compact_messages&quot;,
 &quot;tokens_before&quot;: 128000, &quot;tokens_after&quot;: 64000}
</code></pre>
<p><strong><code>guardrail</code></strong> -- injected when the agent repeats the same failing tool call.
<code>level</code> is <code>"nudge"</code> (2 consecutive identical errors) or <code>"stop"</code> (3+).</p>
<pre><code class="language-json">{&quot;turn&quot;: 7, &quot;type&quot;: &quot;guardrail&quot;, &quot;tool&quot;: &quot;edit_file&quot;, &quot;level&quot;: &quot;nudge&quot;}
</code></pre>
<p><strong><code>truncated_response</code></strong> -- the LLM hit its output token limit mid-response.</p>
<pre><code class="language-json">{&quot;turn&quot;: 4, &quot;type&quot;: &quot;truncated_response&quot;}
</code></pre>
<h2 id="benchmarking-workflow">Benchmarking workflow</h2>
<p>A typical benchmarking setup runs the same set of tasks across different
configurations and compares the reports. For more deterministic results, pass
<code>--seed</code> so the model samples consistently across runs:</p>
<pre><code class="language-sh">swival &quot;task&quot; --seed 42 --report run1.json
</code></pre>
<p>Not all models guarantee identical outputs with the same seed, but it reduces
variance significantly. See <a href="customization.html#seed">Customization</a> for details.</p>
<h3 id="comparing-models">Comparing models</h3>
<pre><code class="language-sh">for model in qwen3-coder-next deepseek-coder-v2; do
    swival &quot;Fix the failing tests in tests/&quot; \
        --model &quot;$model&quot; \
        --report &quot;results/${model}.json&quot;
done
</code></pre>
<h3 id="comparing-settings">Comparing settings</h3>
<pre><code class="language-sh">for temp in 0.2 0.55 0.8; do
    swival &quot;Refactor src/api.py&quot; \
        --temperature &quot;$temp&quot; \
        --report &quot;results/temp-${temp}.json&quot;
done
</code></pre>
<h3 id="evaluating-agentmd-files">Evaluating AGENT.md files</h3>
<pre><code class="language-sh">for variant in minimal detailed strict; do
    cp &quot;agent-variants/${variant}.md&quot; project/AGENT.md
    swival &quot;Add input validation to the CLI&quot; \
        --base-dir project \
        --report &quot;results/agent-${variant}.json&quot;
done
</code></pre>
<h3 id="reading-reports">Reading reports</h3>
<p>Reports are plain JSON. Use <code>jq</code> to pull out what you need:</p>
<pre><code class="language-sh"># Outcome and turn count
jq '{outcome: .result.outcome, turns: .stats.turns}' run1.json

# Total time spent in LLM calls vs tool execution
jq '{llm: .stats.total_llm_time_s, tools: .stats.total_tool_time_s}' run1.json

# Which tools were used and how often
jq '.stats.tool_calls_by_name' run1.json

# All failed tool calls from the timeline
jq '[.timeline[] | select(.type == &quot;tool_call&quot; and .succeeded == false)]' run1.json

# Which skills were activated
jq '.stats.skills_used' run1.json

# Did context management kick in?
jq '{compactions: .stats.compactions, turn_drops: .stats.turn_drops}' run1.json
</code></pre>
<h3 id="comparing-two-runs">Comparing two runs</h3>
<pre><code class="language-sh"># Side-by-side outcome summary
paste &lt;(jq -r '.result.outcome' a.json) &lt;(jq -r '.result.outcome' b.json)

# Turns and tool calls
diff &lt;(jq '{turns: .stats.turns, tools: .stats.tool_calls_total}' a.json) \
     &lt;(jq '{turns: .stats.turns, tools: .stats.tool_calls_total}' b.json)
</code></pre>
<h2 id="what-the-report-doesnt-include">What the report doesn't include</h2>
<p>The report captures the agent's behavior, not the correctness of its output.
Whether the agent actually solved the task -- whether the code compiles, tests
pass, the refactor is sound -- is up to your evaluation harness. The <code>answer</code>
field gives you the agent's final text, and the <code>outcome</code> tells you whether it
finished cleanly, but judging quality is outside the scope of the report.</p>
        </article>
    </div>
    <footer class="site-footer">
        MIT License &middot;
        <a href="https://github.com/swival/swival">GitHub</a>
    </footer>
</body>
</html>